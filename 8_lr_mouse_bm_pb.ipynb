{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "following-buffer",
   "metadata": {},
   "source": [
    "# Low dimensional logistic regression label transfer of variance decomposed Single Cell Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "treated-heading",
   "metadata": {},
   "source": [
    "## Author Notes\n",
    "\n",
    "* The following tutorial describes a method for integrating data by label transfering based on ridge regulariased logistic regression in low-dimensional space. This approach enables us to fit a model on the annotated training/landscape/reference dataset to predict labels of a new dataset and embed these labels onto neighbourhoods in the prediction data. \n",
    "\n",
    "* Logistic regression can (1) be used to classify samples, (2) use different types of data (continuous and descrete measurements)and (3) also be used to assess what variables are useful for classifying samples.\n",
    "\n",
    "#### Current progress: \n",
    "- integrating a latent variable model for neighbourhood construction using factor decomposition. \n",
    "- Integration of BIC metric\n",
    "- Integration of Robustness testing\n",
    "#### Testing required: \n",
    "- Robustness of runs need to be tested by permutation of dependent variables and similairty of outcome tested using MI, RAND or BIC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-priest",
   "metadata": {},
   "source": [
    "# Author information"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "explicit-yesterday",
   "metadata": {},
   "source": [
    "### Created on: 291020; Updated: 050621;\n",
    "### Author: Issac Goh\n",
    "### Proof readers/Testers: Simone Webb , Mariana Quiroga Londo√±o, Anthony Rose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cutting-mirror",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import subprocess\n",
    "\n",
    "#import pkg_resources\n",
    "#required = {'harmonypy','sklearn','scanpy','pandas', 'numpy', 'bbknn', 'scipy', 'matplotlib', 'seaborn' ,'scipy'}\n",
    "#installed = {pkg.key for pkg in pkg_resources.working_set}\n",
    "#missing = required - installed\n",
    "#if missing:\n",
    "#    print(\"Installing missing packages:\" )\n",
    "#    print(missing)\n",
    "#    python = sys.executable\n",
    "#    subprocess.check_call([python, '-m', 'pip', 'install', *missing], stdout=subprocess.DEVNULL)\n",
    "\n",
    "%matplotlib inline\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "from bbknn import bbknn\n",
    "import scipy\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "from geosketch import gs\n",
    "from numpy import cov\n",
    "import scipy.cluster.hierarchy as spc\n",
    "import seaborn as sns; sns.set(color_codes=True)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn\n",
    "import harmonypy as hm\n",
    "from pathlib import Path\n",
    "\n",
    "sc.settings.verbosity = 3  # verbosity: errors (0), warnings (1), info (2), hints (3)\n",
    "sc.settings.set_figure_params(dpi=80, color_map='viridis')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "focal-malta",
   "metadata": {},
   "source": [
    "# Only block to edit in LR script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defined-plumbing",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Introduce variables\n",
    "# Note that this script expects raw data to be in \"non-batch-corrected\" adata.raw.X. \n",
    "\n",
    "# Required: Introduce the path you'd like to save figures or data to \n",
    "save_path = \"/Users/emilyjones/Desktop/HCA/project/script templates/raw counts/lr_mouse_bm_pb\"\n",
    "\n",
    "# Required: Name of first object\n",
    "data1 = \"mouse\"\n",
    "# Provide path to obj1 // landscape/training data\n",
    "Object1 = \"/Users/emilyjones/Desktop/HCA/project/script templates/raw counts/pb_mouse_raw_pseudobulks.h5ad\" #Make sure that this is unprocessed\n",
    "# Provide categorical to join between datasets\n",
    "cat1 = \"batch_id\"\n",
    "\n",
    "# Required: Name of second object\n",
    "data2 = \"fbm\"\n",
    "# Provide path to obj2 // prediction/projection data\n",
    "Object2 = \"/Users/emilyjones/Desktop/HCA/project/script templates/raw counts/FBM_raw_with_meta.h5ad\"  #Make sure that this is unprocessed\n",
    "# Provide categorical to join between datasets\n",
    "cat2 = \"cell.labels\"\n",
    "\n",
    "# Required: LR Model Options\n",
    "penalty='elasticnet' # can be [\"l1\",\"l2\",\"elasticnet\"]\n",
    "sparcity=0.2\n",
    "max_iter = 200 #Increase if experiencing max iter issues\n",
    "l1_ratio = 0.5 #If using elasticnet, tis controls the ratio between l1 and l2\n",
    "\n",
    "# Optional: Batch correction options (this is for correction of eventual combined dataset for data1 and data2)\n",
    "# If you do not have a batch variable for either data1 or data2, please add a \"filler\" column in the relevent adata.obs\n",
    "# for the purposes of batch_correction and batch args below.\n",
    "# e.g., adata.obs[\"whatever\"] = \"something\"; batch=\"whatever\"\n",
    "batch_correction = \"Harmony\" # Will accept Harmony, BBKNN or False as options\n",
    "batch = [\"species_batch\",\"species_batch\",] # Will accept any batch categorical. Comma space a batch categorical for each dataset. Position 1 is for data1, position 2 is for data2\n",
    "\n",
    "# Optional: miscellaneous Options.   \n",
    "subsample_train = False # Samples the training data to the smallest fraction (highly dependent on resolution of input celltype categorical). This corrects for proportional differences between celltype labels of interest in the training data. E.g., training data has 50,000 B cells, 20,000 T cells and 100 HSCs. This function will subsample all training to 100 cells per cell type. \n",
    "subsample_prop = 0.2 # Give this option a proprtion to subsample to(e.g 0.2), if NA given, will subsample to smallest population\n",
    "subsample_predict = False\n",
    "subsample_prop_predict = 0.5\n",
    "remove_non_high_var = True\n",
    "\n",
    "train_x = 'X_pca' # Define the resource to train and predict on, PCA, X or UMAP (#if you wish to use gene expression, train_x = 'X')\n",
    "remove_effect_of_custom_gene_list = 'NA' # \"./cell_cycle_genes.csv\" #remove a custom list of genes from just the variable genes to compute PCA from. Your .csv should have HGNC gene names in the first column to be read in as a vector, any column name is fine.\n",
    "use_raw = True # Do you want to use adata.raw.X (recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555b19a9-1b4c-4eb5-96b4-b947d7deef1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Check if filepaths are good\n",
    "if not os.path.exists(save_path):\n",
    "    os.makedirs(save_path)\n",
    "    \n",
    "if (Path(Object1).is_file() & Path(Object2).is_file()):\n",
    "    print(\"adata file paths detetcted, proceeding to load\")\n",
    "    adata = sc.read(Object1)\n",
    "    adata2 =  sc.read(Object2)\n",
    "    del adata.uns\n",
    "    del adata2.uns\n",
    "else: \n",
    "    raise TypeError(\"one or more .h5ad paths cannot be accessed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b40dee18-c030-42a8-aaa9-7aef492eaa49",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs[\"species_batch\"] = 'mouse ' +  adata.obs[\"batch_id\"].astype(str)\n",
    "adata2.obs[\"species_batch\"] = 'human ' + adata2.obs[\"orig.ident\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ee2c1f-4c99-47ac-a091-8d4c0e2a66bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Move human genes into .var on mouse data\n",
    "adata.var = adata.var.set_index('symbol')\n",
    "adata.var.index.names = [\"index\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18300bc6-cd5d-4319-8a05-bdf5c238a0a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3fe01fa-2098-45f9-bd67-438669d04bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata2.var"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-karen",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Below modules will be tuned into functions in package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "three-wagon",
   "metadata": {},
   "source": [
    "## Combining data and Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regular-tourism",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# altering scanpy setting so that we can save it to our defined directory\n",
    "sc._settings.ScanpyConfig(figdir=save_path)\n",
    "\n",
    "# Combine and pre-process data to match correlations across PCA\n",
    "\n",
    "# Module to detect shape mismatch and alternatively rebuild adata\n",
    "if(use_raw==True):\n",
    "    print('option detected to use raw data, proceeding to check if raw exists and if it matches data.X')\n",
    "    if (hasattr(adata.raw, \"X\")):\n",
    "        try: adata.X =  adata.raw.X  ; print('no mismatch in shape for adata detected')\n",
    "        except: print(\"adata.X shape mismatched with adata.raw.X, proceeding to re-build data\") ; adata = adata.raw.to_adata()\n",
    "    else:\n",
    "        print(\"no raw data detected in adata! proceeding to create raw partition from adata.X\")\n",
    "        adata.raw = adata\n",
    "        \n",
    "    if (hasattr(adata2.raw, \"X\")):\n",
    "        try: adata2.X = adata2.raw.X ; print('no mismatch in shape for adata2 detected')\n",
    "        except: print(\"adata2.X shape mismatched with adata.raw.X, proceeding to re-build data\") ; adata2 = adata2.raw.to_adata()\n",
    "    else:\n",
    "        print(\"no raw data detected in adata! proceeding to create raw partition from adata.X\")\n",
    "        adata2.raw = adata2\n",
    "            \n",
    "# Define intersecting genes between datasets\n",
    "adata_genes = list(adata.var.index)\n",
    "adata2_genes = list(adata2.var.index)\n",
    "keep_SC_genes = list(set(adata_genes) & set(adata2_genes))\n",
    "print(\"keep gene list = \" , len(keep_SC_genes), \"adata1 gene length = \", len(adata_genes) , \"adata2 gene length = \", len(adata2_genes) )\n",
    "\n",
    "# Remove non-intersecting genes (this step will remove cite-seq data if training data is pure RNA seq)\n",
    "adata_intersect1 = adata[:, keep_SC_genes]\n",
    "adata = adata_intersect1\n",
    "adata_intersect2 = adata2[:, keep_SC_genes]\n",
    "adata2 = adata_intersect2\n",
    "\n",
    "# Optional subsampling of training data to \n",
    "if(subsample_train == True):\n",
    "    \n",
    "    if not(subsample_prop==\"NA\"):\n",
    "        print(\"option to subsample by proportion chosen\")\n",
    "        prop = subsample_prop\n",
    "        grouped = data.groupby(cat1)\n",
    "        df = grouped.apply(lambda x: x.sample(frac=prop))\n",
    "        df = df.droplevel(cat1)\n",
    "        keep = df.index\n",
    "        adata = adata[adata.obs.index.isin(keep)]\n",
    "    else:\n",
    "        print(\"subsample by smallest population\")\n",
    "        data = adata.obs\n",
    "        data = data.sample(frac=1).groupby(cat1).head(min(adata.obs.groupby(cat1).size()))\n",
    "        keep = data.index\n",
    "        adata = adata[adata.obs.index.isin(keep)]\n",
    "        \n",
    "# Optional subsampling of training data to \n",
    "if(subsample_predict == True):\n",
    "    if not(subsample_prop_predict==\"NA\"):\n",
    "        print(\"option to subsample by proportion chosen\")\n",
    "        prop = subsample_prop_predict\n",
    "        data = adata2.obs[:]\n",
    "        grouped = data.groupby(cat2)\n",
    "        df = grouped.apply(lambda x: x.sample(frac=prop))\n",
    "        df = df.droplevel(cat2)\n",
    "        keep = df.index\n",
    "        adata2 = adata2[adata2.obs.index.isin(keep)]\n",
    "    else:\n",
    "        print(\"subsample by smallest population\")\n",
    "        data = adata2.obs\n",
    "        data = data.sample(frac=1).groupby(cat2).head(min(adata.obs.groupby(cat2).size()))\n",
    "        keep = data.index\n",
    "        adata2 = adata2[adata2.obs.index.isin(keep)]\n",
    "\n",
    "# Create a common batch column and do simple sanity check for batch variables\n",
    "if not((batch_correction == \"False\") and (len(batch)>1)):\n",
    "    print(\"Batch correction option detected, proceeding to format batch variables\")\n",
    "    batch_var = \"lr_batch\"\n",
    "    adata.obs[\"lr_batch\"] = adata.obs[batch[0]]\n",
    "    adata2.obs[\"lr_batch\"] = adata2.obs[batch[1]]\n",
    "else: raise TypeError(\"Batch correction option detected but requires at least one categorical for each dataset!\")\n",
    "\n",
    "# Create a common obs column in both datasets containing the data origin tag\n",
    "common_cat = \"corr_concat\" \n",
    "adata.obs[common_cat] = adata.obs[cat1].astype(str) + data1\n",
    "adata2.obs[common_cat] = adata2.obs[cat2].astype(str) + data2\n",
    "adata.obs = adata.obs.astype('category')\n",
    "adata2.obs = adata2.obs.astype('category')\n",
    "concat = adata2.concatenate(adata, join='inner',index_unique=None, batch_categories=None)\n",
    "adata = concat[:]\n",
    "sc.pp.normalize_per_cell(adata, counts_per_cell_after=1e4)\n",
    "sc.pp.log1p(adata)\n",
    "sc.pp.highly_variable_genes(adata, min_mean=0.1, max_mean=4)\n",
    "sc.pp.scale(adata, zero_center=False, max_value=None, copy=False) #zero_center=True (densifies output)\n",
    "\n",
    "# Optionally remove genes of known confounding effect from variable list\n",
    "if not (Path(remove_effect_of_custom_gene_list).is_file()):\n",
    "    print(\"Custom gene list option is not selected or path is not readbale, proceeding with no variable removal\")\n",
    "else: \n",
    "    print(\"Custom gene removal list detected, proceeding to remove intersect from variable genes\")\n",
    "    regress_list = pd.read_csv(remove_effect_of_custom_gene_list)\n",
    "    regress_list = regress_list.iloc[:, 0]\n",
    "    adata.var[\"highly_variable\"][adata.var.index.isin(regress_list)] = \"False\"\n",
    "\n",
    "#Optionally remove genes that do not contribute to variance in combined data::Use only if training and predicting withsim reduced data    \n",
    "if(remove_non_high_var==True):\n",
    "    high_var = list(adata.var[\"highly_variable\"][adata.var[\"highly_variable\"]==True])\n",
    "    adata = adata[:, adata.var[\"highly_variable\"].isin(high_var)]   \n",
    "\n",
    "# Now compute PCA\n",
    "sc.pp.pca(adata, n_comps=50, use_highly_variable=True, svd_solver='arpack')\n",
    "\n",
    "# Batch correction options\n",
    "# The script will test later which Harmony values we should use\n",
    "if not(batch_correction == \"False\"):\n",
    "    sc.pp.neighbors(adata, n_neighbors=15, n_pcs=50)    \n",
    "if(batch_correction == \"Harmony\"):\n",
    "    print(\"Commencing harmony\")\n",
    "    # Create hm subset\n",
    "    adata_hm = adata[:]\n",
    "    # Set harmony variables\n",
    "    data_mat = np.array(adata_hm.obsm[\"X_pca\"])\n",
    "    meta_data = adata_hm.obs\n",
    "    vars_use = [batch_var]\n",
    "    # Run Harmony\n",
    "    ho = hm.run_harmony(data_mat, meta_data, vars_use)\n",
    "    res = (pd.DataFrame(ho.Z_corr)).T\n",
    "    res.columns = ['X{}'.format(i + 1) for i in range(res.shape[1])]\n",
    "    # Insert coordinates back into object\n",
    "    adata_hm.obsm[\"X_pca_back\"]= adata_hm.obsm[\"X_pca\"][:]\n",
    "    adata_hm.obsm[\"X_pca\"] = np.array(res)\n",
    "    # Run neighbours\n",
    "    sc.pp.neighbors(adata_hm, n_neighbors=15, n_pcs=50)\n",
    "    adata = adata_hm[:]\n",
    "    del adata_hm\n",
    "elif(batch_correction == \"BBKNN\"):\n",
    "    print(\"Commencing BBKNN\")\n",
    "    sc.external.pp.bbknn(adata, batch_key=batch_var, approx=True, metric='angular', copy=False, n_pcs=50, trim=None, n_trees=10, use_faiss=True, set_op_mix_ratio=1.0, local_connectivity=15) \n",
    "    \n",
    "print(\"adata1 and adata2 are now combined and preprocessed in 'adata' obj - success!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compact-interstate",
   "metadata": {},
   "source": [
    "## Logistic regression function to train data set and transfer labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "israeli-medication",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function require compute power, will take a while \n",
    "\n",
    "def LR_compare(adata, train_x, train_label, subset_predict, subset_train, penalty=penalty, sparcity=sparcity, \n",
    "               col_name='predicted'):\n",
    "\n",
    "    # adata - training+prediction adata object (combined). Pre-processed already\n",
    "    # sparsity - larger sparsity, more bins, more conservative predictions, less accurate. Low sparist for clean output\n",
    "                # A value of 0.2 is reasonable for L2 ridge regression\n",
    "    # penalty - acts as buffer for assigning bins too harshly\n",
    "    # train_x - arg refers to where you would like to derive your training reference from, i.e., GEX (X) or/elif.\n",
    "                # PCA/UMAP in obsm. The two 'if' statements below handle train_x differently based on this\n",
    "                # Based on train_x, the loops below compute 'train_label' (cell type values in training/landscape data) \n",
    "                # and 'predict_x'(prediction data equivalent of train_x)\n",
    "    # train_label - cell type values in training/landscape data\n",
    "    # subset_predict - mandatory subset of predict_x which contains metadata for expression\n",
    "    # subset_train - mandatory subset of train_x which contains metadata for expression\n",
    "    \n",
    "    # Redefine LR parameters 'penalty' and 'sparsity' if you would like to deviate from defaults set above\n",
    "    \n",
    "    # Assign 'lr' as sklearn logistic regression func, with penalty and sparsity defined above\n",
    "    lr = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  max_iter)\n",
    "    \n",
    "    if (penalty == \"l1\"):\n",
    "        lr = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  max_iter, dual = True, solver = 'liblinear')\n",
    "    if (penalty == \"elasticnet\"):\n",
    "        lr = LogisticRegression(penalty = penalty, C = sparcity, max_iter =  max_iter, dual=False,solver = 'saga',l1_ratio=l1_ratio)\n",
    "\n",
    "    if train_x == 'X':\n",
    "        # Define training parameters\n",
    "        train_label = adata.obs[common_cat].values\n",
    "        train_label = train_label[subset_train]\n",
    "        #train_x = adata.X,\n",
    "        # Define prediction parameters\n",
    "        #predict_x = train_x\n",
    "        #train_x = train_x[subset_train, :] # issue line! subset_train = np.array(adata.obs[common_cat].isin(group1))\n",
    "                                           # group1 = (adata.obs[common_cat][adata.obs[common_cat].str.contains(Data1_group)]).unique()\n",
    "                                           # Data1_group = data1 = healthy skin data , adata containing subsetting data to get metadata for expression prediction\n",
    "                                           # adata.X = adata.X[np.array(adata.obs[common_cat].isin(group1)), :]\n",
    "                                           # train_x = train_x[adata.obs[common_cat].isin(group1)]\n",
    "                        \n",
    "        #predict_x = train_x\n",
    "        #predict_x = predict_x[subset_predict]\n",
    "        train_x = adata.X[adata.obs.index.isin(list(adata.obs[subset_train].index))]\n",
    "        predict_x = adata.X[adata.obs.index.isin(list(adata.obs[subset_predict].index))]\n",
    "\n",
    "    elif train_x in adata.obsm.keys():\n",
    "        # Define training parameters\n",
    "        train_label = adata.obs[common_cat].values\n",
    "        train_label = train_label[subset_train]\n",
    "        train_x = adata.obsm[train_x]\n",
    "        predict_x = train_x\n",
    "        train_x = train_x[subset_train, :]\n",
    "        # Define prediction parameters\n",
    "        predict_x = predict_x[subset_predict]\n",
    "        predict_x = pd.DataFrame(predict_x)\n",
    "        predict_x.index = adata.obs[subset_predict].index\n",
    "\n",
    "    # Train predictive model using user defined partition labels (train_x ,train_label, predict_x)\n",
    "    model = lr.fit(train_x, train_label)\n",
    "    lr.fit(train_x, train_label)\n",
    "    predict = lr.predict_proba(predict_x)\n",
    "\n",
    "    # Create prediction table and map to adata.obs (in adata.obs[\"predict\"] in the combined object), for the cells that\n",
    "    # are in predict dataset\n",
    "    predict = lr.predict(predict_x)\n",
    "    predict = pd.DataFrame(predict)\n",
    "    predict.index = adata.obs[subset_predict].index\n",
    "    adata.obs[col_name] = adata.obs.index\n",
    "    adata.obs[col_name] = adata.obs[col_name].map(predict[0])\n",
    "\n",
    "# Function to plot heatmap by percentage\n",
    "def plot_df_heatmap(df, cmap='viridis', title=None, figsize=(7, 7), rotation=90, save=None, **kwargs):\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    im = ax.imshow(df, cmap=cmap, aspect='auto', **kwargs)\n",
    "    if 0 < rotation < 90:\n",
    "        horizontalalignment = 'right'\n",
    "    else:\n",
    "        horizontalalignment = 'center'\n",
    "    plt.xticks(\n",
    "        range(len(df.columns)),\n",
    "        df.columns,\n",
    "        rotation=rotation,\n",
    "        horizontalalignment=horizontalalignment,\n",
    "    )\n",
    "    plt.yticks(range(len(df.index)), df.index)\n",
    "    if title:\n",
    "        fig.suptitle(title)\n",
    "    #fig.colorbar(im)\n",
    "    if save:\n",
    "        plt.savefig(fname=save, bbox_inches='tight', pad_inches=0.1)\n",
    "\n",
    "# Plot probability table by html\n",
    "def cross_table(adata, x, y, normalise=None, highlight=False, subset=None):                                                                                                                                                                                              \n",
    "    \"\"\"Make a cross table comparing two categorical annotations\n",
    "    \"\"\"\n",
    "    x_attr = adata.obs[x]\n",
    "    y_attr = adata.obs[y]\n",
    "    if subset is not None:\n",
    "        x_attr = x_attr[subset]\n",
    "        y_attr = y_attr[subset]\n",
    "    crs_tbl = pd.crosstab(x_attr, y_attr)\n",
    "    if normalise == 'x':\n",
    "        x_sizes = x_attr.groupby(x_attr).size().values\n",
    "        crs_tbl = (crs_tbl.T / x_sizes).round(2).T\n",
    "    elif normalise == 'y':\n",
    "        y_sizes = x_attr.groupby(y_attr).size().values\n",
    "        crs_tbl = (crs_tbl / y_sizes).round(2)\n",
    "    if highlight:\n",
    "        return crs_tbl.style.background_gradient(cmap='viridis', axis=0)\n",
    "    return crs_tbl\n",
    "\n",
    "# Define the separator category in the column of interest, this works by partial matches and enables a-symmetric \n",
    "# comparisons\n",
    "Data1_group = data1\n",
    "Data2_group = data2\n",
    "# Define the common .obs column between concatinated data\n",
    "common_cat = \"corr_concat\"\n",
    "\n",
    "# This block defines subset_predict and subset_train and also runs LR_compare function\n",
    "group1 = (adata.obs[common_cat][adata.obs[common_cat].str.contains(Data1_group)]).unique()\n",
    "group1 = list(group1)\n",
    "group2 = (adata.obs[common_cat][adata.obs[common_cat].str.contains(Data2_group)]).unique()\n",
    "group2 = list(group2)\n",
    "subset_predict = np.array(adata.obs[common_cat].isin(group2))\n",
    "subset_train = np.array(adata.obs[common_cat].isin(group1))\n",
    "train_label = (adata.obs[common_cat][adata.obs[common_cat].isin(group1)]).values\n",
    "\n",
    "LR_compare(adata, train_x, train_label, subset_predict, subset_train, sparcity=sparcity, col_name='predicted')\n",
    "\n",
    "################################################################################################################################################################\n",
    "#HTML cross-table construction\n",
    "\n",
    "# Plotting purposes - Subset data to only contain prediction data with new predicted labels√•\n",
    "# Make adata the predicted data with projected annotations in adata.obs[\"predicted\"]. \"corr_concat\" contains old \n",
    "# cell.labels and dataset name combined \n",
    "common_cat = \"corr_concat\"\n",
    "adata_concat = adata[:]\n",
    "adata = adata[adata.obs[\"predicted\"].isin(group1)]\n",
    "adata = adata[adata.obs[\"corr_concat\"].isin(group2)]\n",
    "\n",
    "# The results, are displayed in a python notebook or rendered as a html (just for quick visualisation, not saving)\n",
    "# This table represents probability (binary cell type assignment) - numbers represent number of cells\n",
    "# Columns contain number of original cells from adata2 that are now labelled using the training dataset, adata1\n",
    "# (as shown on y axis)\n",
    "crs_tbl = cross_table(adata, x = 'predicted', y = common_cat, highlight = True)\n",
    "#crs_tbl\n",
    "\n",
    "################################################################################################################################################################\n",
    "# Heatmap function\n",
    "\n",
    "x = 'predicted'\n",
    "y = common_cat\n",
    "\n",
    "y_attr = adata.obs[y]\n",
    "x_attr = adata.obs[x]\n",
    "crs = pd.crosstab(x_attr, y_attr)\n",
    "crs_tbl = crs\n",
    "for col in crs_tbl :\n",
    "    crs_tbl[col] = crs_tbl[col].div(crs_tbl[col].sum(axis=0)).multiply(100).round(2)\n",
    "# Sort df columns by rows\n",
    "crs_tbl = crs_tbl.sort_values(by =list(crs_tbl.index), axis=1,ascending=False)\n",
    "# Plot_df_heatmap(crs_tbl, cmap='coolwarm', rotation=90, vmin=20, vmax=70)\n",
    "pal = sns.diverging_palette(240, 10, n=10)\n",
    "plt.figure(figsize=(20,15))\n",
    "sns.set(font_scale=0.8)\n",
    "g = sns.heatmap(crs_tbl, cmap=pal, vmin=0, vmax=100, linewidths=1, center=50, square=True, cbar_kws={\"shrink\": 0.3})\n",
    "plt.xlabel(\"Original labels\")\n",
    "plt.ylabel(\"Predicted labels\")\n",
    "#plt.savefig(save_path + \"/LR_predictions.pdf\")\n",
    "#crs_tbl.to_csv(save_path + \"/pre-freq_LR_predictions_supp_table.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "possible-technical",
   "metadata": {},
   "source": [
    "### Proportion of cells per group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considerable-assumption",
   "metadata": {},
   "outputs": [],
   "source": [
    "prop = adata.obs.groupby('predicted').count()\n",
    "prop['percentage'] = (prop.iloc[:,6]/prop.iloc[:,6].sum())*100\n",
    "prop = prop['percentage']\n",
    "prop.to_csv(save_path + \"/pre-freq_predicted_prop.csv\")\n",
    "prop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "super-hammer",
   "metadata": {},
   "source": [
    "## Modules to compute Leiden-LR Consensus"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-pickup",
   "metadata": {},
   "source": [
    "## Clustering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-compatibility",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional clustering \n",
    "res = 5\n",
    "key_add = 'leiden'\n",
    "adata.obs[key_add] = \"nan\"\n",
    "sc.tl.leiden(adata, resolution= res, key_added= key_add, random_state=26, n_iterations=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "successful-shopper",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_prediction = \"clus_prediction\"\n",
    "clusters_reassign = \"leiden\"\n",
    "lr_predicted_col = 'predicted'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thick-blind",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.obs[cluster_prediction] = adata.obs.index\n",
    "for z in adata.obs[clusters_reassign].unique():\n",
    "    df = adata.obs\n",
    "    df = df[(df[clusters_reassign].isin([z]))]\n",
    "    df_count = pd.DataFrame(df[lr_predicted_col].value_counts())\n",
    "    freq_arranged = df_count.index\n",
    "    cat = freq_arranged[0]\n",
    "    df.loc[:,cluster_prediction] = cat\n",
    "    adata.obs.loc[adata.obs[clusters_reassign] == z, [cluster_prediction]] = cat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "excessive-region",
   "metadata": {},
   "source": [
    "### Heatmap showing cells in predicted clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "talented-hours",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a concat col for old + predicted annots\n",
    "adata.obs['annot_clus_prediction_concat'] = adata.obs[cat1].astype(str) + \"_\" + adata.obs['clus_prediction'].astype(str)\n",
    "\n",
    "x='clus_prediction'\n",
    "y = common_cat\n",
    "\n",
    "y_attr = adata.obs[y]\n",
    "x_attr = adata.obs[x]\n",
    "crs = pd.crosstab(x_attr, y_attr)\n",
    "crs_tbl = crs\n",
    "for col in crs_tbl :\n",
    "    crs_tbl[col] = crs_tbl[col].div(crs_tbl[col].sum(axis=0)).multiply(100).round(2)\n",
    "\n",
    "#plot_df_heatmap(crs_tbl, cmap='coolwarm', rotation=90, vmin=20, vmax=70)\n",
    "pal = sns.diverging_palette(240, 10, n=10)\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.set(font_scale=0.8)\n",
    "g = sns.heatmap(crs_tbl, cmap=pal, vmin=0, vmax=100, linewidths=1, center=50, square=True, cbar_kws={\"shrink\": 0.3})\n",
    "plt.xlabel(\"Original labels\")\n",
    "plt.ylabel(\"Predicted labels\")\n",
    "plt.savefig(save_path + \"/LR_predictions_consensus.pdf\")\n",
    "crs_tbl.to_csv(save_path + \"/post-freq_LR_predictions_consensus_supp_table.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tamil-reach",
   "metadata": {},
   "outputs": [],
   "source": [
    "prop = adata.obs.groupby('clus_prediction').count()\n",
    "prop['percentage'] = prop.iloc[:,6]/prop.iloc[:,6].sum()\n",
    "prop = prop['percentage']\n",
    "prop.to_csv(save_path + \"/post-freq_predicted_leiden_consensus_prop.csv\")\n",
    "prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "useful-magazine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a umap to view your current data\n",
    "sc.tl.umap(adata)\n",
    "\n",
    "def generate_colors(col_required):\n",
    "    import random\n",
    "    number_of_colors = col_required\n",
    "    color = [\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)])\n",
    "                 for i in range(number_of_colors)]\n",
    "    return list(color)\n",
    "\n",
    "col = generate_colors(len(adata.obs[\"predicted\"].unique()))\n",
    "sc.pl.umap(adata, color='predicted', palette = col, save = \"prediction_umap.pdf\",size = 20)\n",
    "\n",
    "col = generate_colors(len(adata.obs[\"clus_prediction\"].unique()))\n",
    "sc.pl.umap(adata, color='clus_prediction', palette = col, save = \"clus_prediction_umap.pdf\", size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "excellent-stockholm",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Are any celltypes removed between prediction and cluster consensus step?\n",
    "prediction_set = list(adata.obs['predicted'].unique())\n",
    "clust_prediction_set = list(adata.obs['clus_prediction'].unique())\n",
    "non_intersect = set(prediction_set) ^ set(clust_prediction_set)\n",
    "non_intersect = non_intersect.intersection(prediction_set)\n",
    "\n",
    "if (len(non_intersect)>0):\n",
    "    print(str(len(non_intersect)) + \" labels were omitted during the consensus redistribution step! Please check if these labels are expected to be significantly available in the prediction dataset! Missin labels are::\" )\n",
    "    print(\"################################\")\n",
    "    print(non_intersect)\n",
    "    \n",
    "    # plot umap coloring only labels that were lost in consensus step\n",
    "    sc.pl.umap(adata,color=\"predicted\",groups=non_intersect, save = \"freq_redistribution_omitted_labels_umap.pdf\")\n",
    "else: print(\"No labels were omitted!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "presidential-grounds",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate Rand and MI\n",
    "rand = sklearn.metrics.adjusted_rand_score(list(adata.obs['predicted']), list(adata.obs['clus_prediction']))\n",
    "mi = sklearn.metrics.adjusted_mutual_info_score(list(adata.obs['predicted']), list(adata.obs['clus_prediction']), \n",
    "                                                average_method='arithmetic')\n",
    "\n",
    "# test concordance between the predicted labels by cell and the consensus clusters\n",
    "if ((rand<0.8) | (mi<0.8)):\n",
    "    print(\"Your concordance between predicted and consensus leiden labels are weak, please attempt reclustering at higher resolution and running consensus again\")\n",
    "    print(\"Adj Rand extimate = \" + str(rand) + \"  \" + \"Mutual Information score=\" + str(mi))\n",
    "else: \n",
    "    print(\"success\")\n",
    "    print(\"You have achieved good consensus between predicted labels and consensus leiden labels\")\n",
    "    print(\"Adj Rand extimate = \" + str(rand) + \"  \" + \"Mutual Information score=\" + str(mi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anticipated-black",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: if low could be due to removed celltypes or may be expected if the cells are quite different such as from different organs, \n",
    "# plot cells to check groups and see if group together nicely or all over "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latest-robert",
   "metadata": {},
   "source": [
    "# Save dataframe with new labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "complex-hypothetical",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = adata.obs[[\"predicted\", \"leiden\", \"clus_prediction\"]]\n",
    "df = df.rename(columns={\"predicted\": \"pre_freq_clus_prediction\", \"clus_prediction\": \"post_freq_clus_prediction\"})\n",
    "df.to_csv(save_path + \"/final_lr_metadata_for_adata2.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b45ef3-39bc-4534-a409-72336313280a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5441ad1f-8541-4b3a-8a67-0f95d2353ede",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python_env",
   "language": "python",
   "name": "python_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
